# -*- coding: utf-8 -*-
"""DSProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VpXtnklH_SQjoLgewZ1SAE4oal7BJVzl

# Libraries
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras import layers, models
from tensorflow.keras.applications import VGG16
from tensorflow.keras.utils import to_categorical
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from matplotlib import pyplot
import numpy as np
import os
import gzip
import shutil
import struct

"""# Extracting and Reading the data

"""

def extract_gz(file_path, output_path):
    with gzip.open(file_path, 'rb') as f_in:
        with open(output_path, 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)

extract_gz('/content/train-images-idx3-ubyte_2.gz', 'train-images-idx3-ubyte')
extract_gz('/content/train-labels-idx1-ubyte_2.gz', 'train-labels-idx1-ubyte')
extract_gz('/content/t10k-images-idx3-ubyte.gz', 't10k-images-idx3-ubyte')
extract_gz('/content/t10k-labels-idx1-ubyte.gz', 't10k-labels-idx1-ubyte')

def read_idx(file_path):
    with open(file_path, 'rb') as f:
        magic = int.from_bytes(f.read(4), 'big')
        num_items = int.from_bytes(f.read(4), 'big')
        rows = int.from_bytes(f.read(4), 'big') if magic == 2051 else None
        cols = int.from_bytes(f.read(4), 'big') if magic == 2051 else None

        if rows and cols:
            data = np.frombuffer(f.read(), dtype=np.uint8).reshape(num_items, rows, cols)
        else:
            data = np.frombuffer(f.read(), dtype=np.uint8)

        return data

train_images_path = 'train-images-idx3-ubyte'
train_labels_path = 'train-labels-idx1-ubyte'
test_images_path = 't10k-images-idx3-ubyte'
test_labels_path = 't10k-labels-idx1-ubyte'

x_train = read_idx(train_images_path)
y_train= read_idx(train_labels_path)
x_test = read_idx(test_images_path)
y_test = read_idx(test_labels_path)

"""# Sample of Data

"""

for i in range(9):
	pyplot.subplot(3 ,3 , i+1)
	pyplot.imshow(x_train[i],cmap=pyplot.get_cmap('gray'))
pyplot.show()

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

"""**Dataset Description:**
Fashion MNIST is a dataset of grayscale images, designed to serve as a replacement for the classic MNIST handwritten digits dataset. It contains images of 10 different categories of clothing items.

**Objective of the Project :**
The goal of this project is to design and train a deep learning model from scratch to classify images from the Fashion MNIST dataset into their respective categories with high accuracy.

**Preprocessing**
"""

# Normalize images to range [0, 1]
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

# Add channel dimension (grayscale to single channel)
x_train = np.expand_dims(x_train, axis=-1)
x_test = np.expand_dims(x_test, axis=-1)

# Resize images to 64x64x3
x_train = tf.image.resize(x_train, [64, 64])
x_test = tf.image.resize(x_test, [64, 64])

# Convert grayscale to 3-channel RGB
x_train = tf.image.grayscale_to_rgb(x_train)
x_test = tf.image.grayscale_to_rgb(x_test)

# Convert labels to one-hot encoding
y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)

"""# Pretrained-model"""

def build_vgg16_fashion_mnist():
    # Load the VGG16 model without the top layer (ImageNet weights)
    base_model = VGG16(weights="imagenet",classes=10, include_top=False, input_shape=(64, 64, 3))

    # Freeze the base model layers
    base_model.trainable = False

    # Add custom classification layers
    model = models.Sequential([
        base_model,
        layers.Flatten(),
        layers.Dense(128, activation="relu"),  # Smaller dense layer
        layers.Dropout(0.5),
        layers.Dense(10, activation="softmax")  # 10 classes for Fashion MNIST
    ])
    return model

# Build the model
model = build_vgg16_fashion_mnist()

# Compile the model
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])

# Print model summary
model.summary()

# Train the model with a smaller batch size
model.fit(
    x_train, y_train,
    batch_size=16,  # Reduced batch size to minimize memory usage
    epochs=10,
    validation_data=(x_test, y_test)
)

"""#Pretrained-model Evaluation"""

class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
               "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]

# Evaluate the model
pretrained_test_accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f"Test Accuracy: {pretrained_test_accuracy[1] * 100:.2f}%")

# Predict on test data
y_pred = np.argmax(model.predict(x_test), axis=1)
y_true = np.argmax(y_test, axis=1)

# Classification report
report = classification_report(y_true, y_pred, target_names=class_names)
print("\nClassification Report:\n", report)

# Confusion matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""# Custom model

**Reading the data**
"""

x_train = read_idx(train_images_path)
y_train= read_idx(train_labels_path)
x_test = read_idx(test_images_path)
y_test = read_idx(test_labels_path)

"""**Preprocessing**"""

# Normalize the pixel values to range [0, 1]
x_train = x_train / 255.0
x_test = x_test / 255.0

# Reshape data to include a single channel (grayscale)
x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

model = Sequential([
    # First convolutional layer
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D((2, 2)),

    # Second convolutional layer
    #Conv2D(64, (3, 3), activation='relu'),
    #MaxPooling2D((2, 2)),

    # Flatten the feature maps into a single vector
    Flatten(),

    # Fully connected layer
    Dense(128, activation='relu'),

    # Dropout for regularization
    Dropout(0.5),

    # Output layer with 10 units (for 10 classes)
    Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history = model.fit(
    x_train, y_train,
    epochs=10,  # You can adjust the number of epochs
    batch_size=64,
    validation_data=(x_test, y_test)
)

"""#Custom model evaluation"""

# Evaluate the model
custom_test_accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f"Test Accuracy: {custom_test_accuracy[1] * 100:.2f}%")

# Predict on test data
y_pred = np.argmax(model.predict(x_test), axis=1)  # Predicted classes

# Use `y_test` directly as `y_true` if it's in integer-label form
y_true = y_test if len(y_test.shape) == 1 else np.argmax(y_test, axis=1)

# Classification report
report = classification_report(y_true, y_pred, target_names=class_names)
print("\nClassification Report:\n", report)

# Confusion matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""#Pretrained model accuracy VS Custom model accuracy

"""

print(f"pretrained Test Accuracy: {pretrained_test_accuracy[1] * 100:.2f}%")
print(f"Custom Test Accuracy: {custom_test_accuracy[1] * 100:.2f}%")